{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pre-trained word embeddings for the French language\n",
    "fasttext_model = KeyedVectors.load_word2vec_format('cc.fr.300.vec', binary=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.tsv', sep='\\t')\n",
    "df_test = pd.read_csv('test.tsv', sep='\\t')\n",
    "# 'url' attribute not used for classification\n",
    "df.drop(['url'], axis=1, inplace=True)\n",
    "df_test.drop(['url'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/bina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/bina/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "french_stopwords = stopwords.words(\"french\") + list(string.punctuation) + [\"''\", \"``\", \"...\", \"’\", \"``\", \"«\", \"»\", \"``\"]\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens] # convert to lower case\n",
    "    tokens = [token for token in tokens if token.isalpha()] # remove punctuation\n",
    "    tokens = [token for token in tokens if token not in french_stopwords]  # remove stopwords\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95th percentile length of combined tokens: 1003\n"
     ]
    }
   ],
   "source": [
    "# tokenizing text to calculate word embeddings\n",
    "df_text = df['text'].apply(tokenize)\n",
    "df_headline = df['headline'].apply(tokenize)\n",
    "df['combined_tokens'] = df_text + df_headline\n",
    "df['length'] = df['combined_tokens'].apply(len) # used later to define padding\n",
    "\n",
    "df_test_text = df_test['text'].apply(tokenize)\n",
    "df_test_headline = df_test['headline'].apply(tokenize)\n",
    "df_test['combined_tokens'] = df_test_text + df_test_headline\n",
    "df_test['length'] = df_test['combined_tokens'].apply(len) # used later to define padding\n",
    "\n",
    "max_length = int(np.percentile(df['length'], 95))  # Using 95th percentile to avoid outliers\n",
    "print(f\"95th percentile length of combined tokens: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tokens(tokens, max_length):\n",
    "    tokens = tokens[:max_length] + ['<pad>'] * max(0, max_length - len(tokens))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding needed so we have uniform dimension of word embeddings\n",
    "df['padded_tokens'] = df['combined_tokens'].apply(lambda x: pad_tokens(x, max_length))\n",
    "df_test['padded_tokens'] = df_test['combined_tokens'].apply(lambda x: pad_tokens(x, max_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer labels for categories\n",
    "label_encoder = LabelEncoder()\n",
    "df['encoded_category'] = label_encoder.fit_transform(df['category'])\n",
    "df_test['encoded_category'] = label_encoder.transform(df_test['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = label_encoder.classes_\n",
    "encoded_values = list(range(len(category_names)))\n",
    "category_dict = dict(zip(encoded_values, category_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "      <th>combined_tokens</th>\n",
       "      <th>length</th>\n",
       "      <th>padded_tokens</th>\n",
       "      <th>encoded_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>L'Ouganda à l'assaut des \"fimbu\" de la RDC</td>\n",
       "      <td>L'Ouganda, placé 79e au classement FIFA le 4 a...</td>\n",
       "      <td>[placé, classement, fifa, avril, a, pourtant, ...</td>\n",
       "      <td>184</td>\n",
       "      <td>[placé, classement, fifa, avril, a, pourtant, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Stopper la détérioration de l’environnement po...</td>\n",
       "      <td>La responsable de la biodiversité des Nations ...</td>\n",
       "      <td>[responsable, biodiversité, nations, unies, es...</td>\n",
       "      <td>321</td>\n",
       "      <td>[responsable, biodiversité, nations, unies, es...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>Coupe d'Afrique des nations 2022 : le sélectio...</td>\n",
       "      <td>Le sélectionneur de la Sierra Leone, John Keis...</td>\n",
       "      <td>[sélectionneur, sierra, leone, john, keister, ...</td>\n",
       "      <td>172</td>\n",
       "      <td>[sélectionneur, sierra, leone, john, keister, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>Tissus africains : pourquoi les teintureries h...</td>\n",
       "      <td>Depuis plus de six siècles, une vaste zone sit...</td>\n",
       "      <td>[depuis, plus, six, siècles, vaste, zone, situ...</td>\n",
       "      <td>438</td>\n",
       "      <td>[depuis, plus, six, siècles, vaste, zone, situ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Les revenus pendant la pandémie des dix hommes...</td>\n",
       "      <td>Pendant la pandémie de coronavirus, la richess...</td>\n",
       "      <td>[pendant, pandémie, coronavirus, richesse, com...</td>\n",
       "      <td>350</td>\n",
       "      <td>[pendant, pandémie, coronavirus, richesse, com...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>sports</td>\n",
       "      <td>C1: Le PSG va disputer sa toute première finale</td>\n",
       "      <td>Le Paris SG s'est qualifié pour sa toute premi...</td>\n",
       "      <td>[paris, sg, qualifié, toute, première, finale,...</td>\n",
       "      <td>133</td>\n",
       "      <td>[paris, sg, qualifié, toute, première, finale,...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>sports</td>\n",
       "      <td>Aubameyang invité à rejoindre un \"club plus am...</td>\n",
       "      <td>L'attaquant gabonais d'Arsenal, dont le contra...</td>\n",
       "      <td>[gabonais, dont, contrat, doit, expirer, fin, ...</td>\n",
       "      <td>150</td>\n",
       "      <td>[gabonais, dont, contrat, doit, expirer, fin, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473</th>\n",
       "      <td>politics</td>\n",
       "      <td>Guerre Ukraine - Russie : qui est Sergey Surov...</td>\n",
       "      <td>La désignation de Sergey Surovikin pour dirige...</td>\n",
       "      <td>[désignation, sergey, surovikin, diriger, russ...</td>\n",
       "      <td>499</td>\n",
       "      <td>[désignation, sergey, surovikin, diriger, russ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>technology</td>\n",
       "      <td>Svetlana Jitomirskaya, la mathématicienne à l'...</td>\n",
       "      <td>Il existe un type de papillon qui captive le p...</td>\n",
       "      <td>[existe, type, papillon, captive, professeur, ...</td>\n",
       "      <td>871</td>\n",
       "      <td>[existe, type, papillon, captive, professeur, ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1475</th>\n",
       "      <td>sports</td>\n",
       "      <td>Maxwel Cornet rêve d'une finale avec le Sénégal</td>\n",
       "      <td>Il est l'arme secrète de la Côte d'Ivoire et i...</td>\n",
       "      <td>[secrète, côte, rêve, finale, sénégal, maxwel,...</td>\n",
       "      <td>19</td>\n",
       "      <td>[secrète, côte, rêve, finale, sénégal, maxwel,...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1476 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0         sports         L'Ouganda à l'assaut des \"fimbu\" de la RDC   \n",
       "1       business  Stopper la détérioration de l’environnement po...   \n",
       "2         sports  Coupe d'Afrique des nations 2022 : le sélectio...   \n",
       "3       business  Tissus africains : pourquoi les teintureries h...   \n",
       "4       business  Les revenus pendant la pandémie des dix hommes...   \n",
       "...          ...                                                ...   \n",
       "1471      sports    C1: Le PSG va disputer sa toute première finale   \n",
       "1472      sports  Aubameyang invité à rejoindre un \"club plus am...   \n",
       "1473    politics  Guerre Ukraine - Russie : qui est Sergey Surov...   \n",
       "1474  technology  Svetlana Jitomirskaya, la mathématicienne à l'...   \n",
       "1475      sports    Maxwel Cornet rêve d'une finale avec le Sénégal   \n",
       "\n",
       "                                                   text  \\\n",
       "0     L'Ouganda, placé 79e au classement FIFA le 4 a...   \n",
       "1     La responsable de la biodiversité des Nations ...   \n",
       "2     Le sélectionneur de la Sierra Leone, John Keis...   \n",
       "3     Depuis plus de six siècles, une vaste zone sit...   \n",
       "4     Pendant la pandémie de coronavirus, la richess...   \n",
       "...                                                 ...   \n",
       "1471  Le Paris SG s'est qualifié pour sa toute premi...   \n",
       "1472  L'attaquant gabonais d'Arsenal, dont le contra...   \n",
       "1473  La désignation de Sergey Surovikin pour dirige...   \n",
       "1474  Il existe un type de papillon qui captive le p...   \n",
       "1475  Il est l'arme secrète de la Côte d'Ivoire et i...   \n",
       "\n",
       "                                        combined_tokens  length  \\\n",
       "0     [placé, classement, fifa, avril, a, pourtant, ...     184   \n",
       "1     [responsable, biodiversité, nations, unies, es...     321   \n",
       "2     [sélectionneur, sierra, leone, john, keister, ...     172   \n",
       "3     [depuis, plus, six, siècles, vaste, zone, situ...     438   \n",
       "4     [pendant, pandémie, coronavirus, richesse, com...     350   \n",
       "...                                                 ...     ...   \n",
       "1471  [paris, sg, qualifié, toute, première, finale,...     133   \n",
       "1472  [gabonais, dont, contrat, doit, expirer, fin, ...     150   \n",
       "1473  [désignation, sergey, surovikin, diriger, russ...     499   \n",
       "1474  [existe, type, papillon, captive, professeur, ...     871   \n",
       "1475  [secrète, côte, rêve, finale, sénégal, maxwel,...      19   \n",
       "\n",
       "                                          padded_tokens  encoded_category  \n",
       "0     [placé, classement, fifa, avril, a, pourtant, ...                 3  \n",
       "1     [responsable, biodiversité, nations, unies, es...                 0  \n",
       "2     [sélectionneur, sierra, leone, john, keister, ...                 3  \n",
       "3     [depuis, plus, six, siècles, vaste, zone, situ...                 0  \n",
       "4     [pendant, pandémie, coronavirus, richesse, com...                 0  \n",
       "...                                                 ...               ...  \n",
       "1471  [paris, sg, qualifié, toute, première, finale,...                 3  \n",
       "1472  [gabonais, dont, contrat, doit, expirer, fin, ...                 3  \n",
       "1473  [désignation, sergey, surovikin, diriger, russ...                 2  \n",
       "1474  [existe, type, papillon, captive, professeur, ...                 4  \n",
       "1475  [secrète, côte, rêve, finale, sénégal, maxwel,...                 3  \n",
       "\n",
       "[1476 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating vocabulary of all tokens\n",
    "all_tokens = [token for tokens in df['padded_tokens'] for token in tokens]\n",
    "vocab = set(all_tokens)\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.texts[idx]\n",
    "        indices = [word_to_index.get(token, len(vocab)) for token in tokens]\n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df['padded_tokens'].tolist(), df['encoded_category'].tolist(), test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(X_train, y_train)\n",
    "val_dataset = TextDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 300  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41680"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embedding matrix to use in the CNN Embedding layer\n",
    "embedding_matrix = np.zeros((vocab_size, embed_dim))\n",
    "for word, idx in word_to_index.items():\n",
    "    if word in fasttext_model.key_to_index:\n",
    "        embedding_matrix[idx] = fasttext_model[word]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N - batch size\n",
    "\n",
    "max_length - maximum length of a text sequence\n",
    "\n",
    "vocab_size - size of the vocabulary\n",
    "\n",
    "embed_dim - dimensionality of word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Text(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes, kernel_num=100, kernel_sizes=[3, 4, 5], dropout=0.5, embedding_matrix=None):\n",
    "        super(CNN_Text, self).__init__()\n",
    "        if embedding_matrix is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=False)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, kernel_num, (K, embed_dim)) for K in kernel_sizes])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * kernel_num, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).unsqueeze(1)  # (N, 1, max_length, embed_dim)\n",
    "        x = [nn.functional.relu(conv(x)).squeeze(3) for conv in self.convs]  # [(N, kernel_num, W), ...]*len(kernel_sizes)\n",
    "        x = [nn.functional.max_pool1d(line, line.size(2)).squeeze(2) for line in x]  # [(N, kernel_num), ...]*len(kernel_sizes)\n",
    "        x = torch.cat(x, 1)  # (N, kernel_num * len(kernel_sizes))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc(x)  # (N, num_classes)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_Text(vocab_size, embed_dim, num_classes, embedding_matrix=embedding_matrix)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training with early stopping\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, patience=3):\n",
    "    best_val_acc = 0.0\n",
    "    no_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_acc = correct / total\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "\n",
    "        if no_improvement >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {running_loss / len(train_loader)}, Val Loss: {val_loss / len(val_loader)}, Val Acc: {val_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.4159768223762512, Val Loss: 1.317355751991272, Val Acc: 0.4594594594594595\n",
      "Epoch 2, Train Loss: 1.0031138375401496, Val Loss: 0.9845913529396058, Val Acc: 0.7364864864864865\n",
      "Epoch 3, Train Loss: 0.6285928912460804, Val Loss: 0.7737149596214294, Val Acc: 0.75\n",
      "Epoch 4, Train Loss: 0.38354796059429647, Val Loss: 0.6965559154748917, Val Acc: 0.7871621621621622\n",
      "Epoch 5, Train Loss: 0.22943217866122723, Val Loss: 0.6624760389328003, Val Acc: 0.8006756756756757\n",
      "Epoch 6, Train Loss: 0.11999846268445254, Val Loss: 0.6622957706451416, Val Acc: 0.7871621621621622\n",
      "Epoch 7, Train Loss: 0.07272455333732068, Val Loss: 0.6860948741436005, Val Acc: 0.8006756756756757\n",
      "Early stopping at epoch 8\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.7034702569246292, Test Acc: 0.7871621621621622\n"
     ]
    }
   ],
   "source": [
    "test_acc = correct / total\n",
    "print(f\"Test Loss: {test_loss / len(val_loader)}, Test Acc: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['business', 'health', 'politics', 'sports', 'technology'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_val, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[63,  4,  8,  1,  1],\n",
       "       [ 4, 59,  1,  1,  1],\n",
       "       [15,  2, 45,  0,  2],\n",
       "       [ 2,  4,  1, 60,  1],\n",
       "       [ 6,  2,  1,  6,  6]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
